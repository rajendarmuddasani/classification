{"cells":[{"cell_type":"markdown","id":"f408a6e9-81f0-4544-ad90-0bec6beabb45","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"1045f40c-cb75-495b-aa8e-2f5c27da808d","metadata":{},"outputs":[],"source":["# **K Nearest Neighbor**\n"]},{"cell_type":"markdown","id":"0ec76d0a-1e9e-4aa4-80af-e32771017f4f","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"378ca13e-e127-4bb6-98d6-02f0c18533a0","metadata":{},"outputs":[],"source":["In this lab, you will learn about and practice the K Nearest Neighbor (KNN) model. KNN is a straightforward but very effective model that can be used for both classification and regression tasks. If the feature space is not very large, KNN can be a high-interpretable model because you can explain and understand how a prediction is made by looking at its nearest neighbors.\n"]},{"cell_type":"markdown","id":"971a1910-a728-4d2d-8d34-1ccf37dc9336","metadata":{},"outputs":[],"source":["We will be using a tumor sample dataset containing lab test results about tumor samples. The objective is to classify whether a tumor is malicious (cancer) or benign. As such, it is a typical binary classification task.\n"]},{"cell_type":"markdown","id":"a5c63da6-05a0-404b-96a3-3446ee900340","metadata":{},"outputs":[],"source":["## Objectives\n"]},{"cell_type":"markdown","id":"a7a389fb-1572-4edd-9d6f-5b0b6efae131","metadata":{},"outputs":[],"source":["After completing this lab, you will be able to:\n"]},{"cell_type":"markdown","id":"557667e7-9548-4a25-b0cf-c59f2f8a6627","metadata":{},"outputs":[],"source":["* Train KNN models with different neighbor hyper-parameters\n","* Evaluate KNN models on classification tasks\n","* Tune the number of neighbors and find the optimized one for a specific task\n"]},{"cell_type":"markdown","id":"0fa89bf7-80d8-4fdc-8f79-ff3ad906217f","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"0913941d-ac0f-4bfa-8786-456201da9d5e","metadata":{},"outputs":[],"source":["First, let's install `seaborn` for visualization tasks and import required libraries for this lab.\n"]},{"cell_type":"code","id":"137721b0-d641-406b-8ee2-ef209e07c6bd","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. \n# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\".\n\n!pip install pandas\n!pip install numpy\n!pip install matplotlib\n!pip install seaborn\n!pip install scikit-learn"]},{"cell_type":"code","id":"7a0cdecb-1339-4d2c-a908-125172340917","metadata":{},"outputs":[],"source":["import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# Evaluation metrics related methods\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"]},{"cell_type":"code","id":"8b7bb374-8dab-46f9-b469-69cce5265502","metadata":{},"outputs":[],"source":["# Define a random seed to reproduce any random process\nrs = 123"]},{"cell_type":"code","id":"f3d427e3-d9e5-4fbe-9d95-9c1bcd4bbab4","metadata":{},"outputs":[],"source":["# Ignore any deprecation warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) "]},{"cell_type":"markdown","id":"eb241c52-bace-49ab-a63e-726b642d0bfb","metadata":{},"outputs":[],"source":["## Load and explore the tumor sample dataset\n"]},{"cell_type":"markdown","id":"8afbc05f-a98b-4d1e-854f-0a091d401a6f","metadata":{},"outputs":[],"source":["We first load the dataset `tumor.csv` as a Pandas dataframe:\n"]},{"cell_type":"code","id":"1808e96e-30af-4f82-807b-2402854f53b2","metadata":{},"outputs":[],"source":["# Read datast in csv format\ndataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/tumor.csv\"\ntumor_df = pd.read_csv(dataset_url)"]},{"cell_type":"markdown","id":"db087835-22c6-4d5f-b9e5-f4cebfe2ff16","metadata":{},"outputs":[],"source":["Then, let's quickly take a look at the head of the dataframe.\n"]},{"cell_type":"code","id":"fd940dd6-3509-405c-9b5c-b958fbfa1536","metadata":{},"outputs":[],"source":["tumor_df.head()"]},{"cell_type":"markdown","id":"e8abe804-9ff2-4a8d-a8d6-13acaa66629c","metadata":{},"outputs":[],"source":["And, display its columns.\n"]},{"cell_type":"code","id":"e2d7be8d-1241-4166-93f1-ab8661c796fc","metadata":{},"outputs":[],"source":["tumor_df.columns"]},{"cell_type":"markdown","id":"d68261e2-2d55-4108-8e3c-51f0f40a6714","metadata":{},"outputs":[],"source":["Each observation in this dataset contains lab test results about a tumor sample, such as clump or shapes. Based on these lab test results or features, we want to build a classification model to predict if this tumor sample is malicious (cancer) or benign. The target variable `y` is specified in the `Class` column.\n"]},{"cell_type":"markdown","id":"49e8c789-cb73-4314-82c6-3d225b999ce2","metadata":{},"outputs":[],"source":["Then, let's split the dataset into input `X` and output `y`:\n"]},{"cell_type":"code","id":"31ac9b76-66bc-4779-b6e3-9d522cc62edd","metadata":{},"outputs":[],"source":["X = tumor_df.iloc[:, :-1]\ny = tumor_df.iloc[:, -1:]"]},{"cell_type":"markdown","id":"9fa4e8fb-fc39-40f8-9494-eaa45ce7cde5","metadata":{},"outputs":[],"source":["And, we first check the statistics summary of features in `X`:\n"]},{"cell_type":"code","id":"3f7b030c-df02-4f5e-90be-dd06eb94c77e","metadata":{},"outputs":[],"source":["X.describe()"]},{"cell_type":"markdown","id":"3e3e0cc7-5fe5-4803-93cc-e1452a9a305c","metadata":{},"outputs":[],"source":["As we can see from the above cell output, all features are numeric and ranged between 1 to 10. This is very convenient as we do not need to scale the feature values as they are already in the same range.\n"]},{"cell_type":"markdown","id":"ad110fc2-c361-414c-836d-e6ebf0222d2e","metadata":{},"outputs":[],"source":["Next, let's check the class distribution of output `y`:\n"]},{"cell_type":"code","id":"261a1bbd-0244-4308-8196-764ca01d17fb","metadata":{},"outputs":[],"source":["y.value_counts(normalize=True)"]},{"cell_type":"code","id":"7834a43a-731f-4a9e-8574-dfd774a0ee7f","metadata":{},"outputs":[],"source":["y.value_counts().plot.bar(color=['green', 'red'])"]},{"cell_type":"markdown","id":"7ab5d7bf-ddf7-4a4a-bfba-81ddd60f25fc","metadata":{},"outputs":[],"source":["We have about 65% benign tumors (`Class = 0`) and 35% cancerous tumors (`Class = 1`), which is not a very imbalanced class distribution. \n"]},{"cell_type":"markdown","id":"4e371515-f8a3-4a9f-9c84-170fd8729ffc","metadata":{},"outputs":[],"source":["## Split training and testing datasets\n"]},{"cell_type":"code","id":"6a4dd15f-7d9d-413a-8aed-5368faee5fe8","metadata":{},"outputs":[],"source":["# Split 80% as training dataset\n# and 20% as testing dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"]},{"cell_type":"markdown","id":"8af99a6f-e505-4c8b-a8a2-de107fc8edb3","metadata":{},"outputs":[],"source":["## Train and evaluate a KNN classifier with the number of neighbors set to 2\n"]},{"cell_type":"markdown","id":"52801c62-50f6-42b2-b0c6-3e0c4e670374","metadata":{},"outputs":[],"source":["Training a KNN classifier is very similar to training other classifiers in `sklearn`, we first need to define a `KNeighborsClassifier` object. Here we use `n_neighbors=2` argument to specify how many neighbors will be used for prediction, and we keep other arguments to be their default values.\n"]},{"cell_type":"code","id":"1040ed56-e27e-4fca-b897-690e0878707f","metadata":{},"outputs":[],"source":["# Define a KNN classifier with `n_neighbors=2`\nknn_model = KNeighborsClassifier(n_neighbors=2)"]},{"cell_type":"markdown","id":"e5a7ab63-2a6e-480a-aa8d-306334eb9ae7","metadata":{},"outputs":[],"source":["Then we can train the model with `X_train` and `y_train`, and we use ravel() method to convert the data frame `y_train` to a vector.\n"]},{"cell_type":"code","id":"4509284a-94cb-48f0-a3af-256d3d5eb6b9","metadata":{},"outputs":[],"source":["knn_model.fit(X_train, y_train.values.ravel())"]},{"cell_type":"markdown","id":"a045c817-467a-49b2-a077-d0f8ef778e5f","metadata":{},"outputs":[],"source":["And, we can make predictions on the `X_test` dataframe.\n"]},{"cell_type":"code","id":"f6e3a69d-e0bd-4afc-a7e3-4e2714011aa4","metadata":{},"outputs":[],"source":["preds = knn_model.predict(X_test)"]},{"cell_type":"markdown","id":"3a02e7ca-84ed-4273-9b50-2945d2ec8c48","metadata":{},"outputs":[],"source":["To evaluate the KNN classifier, we provide a pre-defined method to return the commonly used evaluation metrics such as accuracy, recall, precision, f1score, and so on, based on the true classes in the 'y_test' and model predictions.\n"]},{"cell_type":"code","id":"926470c8-dc9c-4df9-ada8-87851ba22b65","metadata":{},"outputs":[],"source":["def evaluate_metrics(yt, yp):\n    results_pos = {}\n    results_pos['accuracy'] = accuracy_score(yt, yp)\n    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp, average='binary')\n    results_pos['recall'] = recall\n    results_pos['precision'] = precision\n    results_pos['f1score'] = f_beta\n    return results_pos"]},{"cell_type":"code","id":"1371c29a-be23-4359-965e-1c4d78f80371","metadata":{},"outputs":[],"source":["evaluate_metrics(y_test, preds)"]},{"cell_type":"markdown","id":"b44ab57d-dbff-4217-994a-0b16c44765fe","metadata":{},"outputs":[],"source":["We can see that there is a great classification performance on the tumor sample dataset. This means the KNN model can effectively recognize cancerous tumors.\n","Next, it's your turn to try a different number of neighbors to see if we could get even better performance.\n"]},{"cell_type":"markdown","id":"9823831c-a34f-4143-9d69-c287b3bdabd9","metadata":{},"outputs":[],"source":["## Coding exercise: Train and evaluate a KNN classifier with number of neighbors set to 5\n"]},{"cell_type":"markdown","id":"f8bf53f9-261c-4388-bddd-a963bec2ad15","metadata":{},"outputs":[],"source":["First, define a KNN classifier with KNeighborsClassifier class:\n"]},{"cell_type":"code","id":"f992f041-b220-4f8e-8044-c2c79c6d31a9","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"b005c354-90d0-475c-8503-e418990591b4","metadata":{},"outputs":[],"source":["Then train the model with `X_train` and `y_train`:\n"]},{"cell_type":"code","id":"df310929-a63a-4913-b7c3-763844f335c3","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"47438149-29c6-4391-8217-8003c4450eae","metadata":{},"outputs":[],"source":["And, make predictions on `X_test` dataframe:\n"]},{"cell_type":"code","id":"92b0f387-7f4a-432c-b0bd-964a53a0a20e","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"f2f65f5d-d8f4-48fc-96cc-7441f05cacb5","metadata":{},"outputs":[],"source":["At last, you can evaluate your KNN model with provided `evaluate_metrics()` method.\n"]},{"cell_type":"markdown","id":"738dfd85-a51c-4297-a326-f5c4f295a9e8","metadata":{},"outputs":[],"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","model = KNeighborsClassifier(n_neighbors=5)\n","model.fit(X_train, y_train.values.ravel())\n","preds = model.predict(X_test)\n","evaluate_metrics(y_test, preds)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"33161f7d-39aa-48d5-ad44-9435def6c572","metadata":{},"outputs":[],"source":["## Tune the number of neighbors to find the optmized one\n"]},{"cell_type":"markdown","id":"7385d859-09f3-48b7-8ee6-150a5ce5e98c","metadata":{},"outputs":[],"source":["OK, you may wonder which `n_neighbors` argument may give you the best classification performance. We can try different `n_neighbors` (the K value) and check which `K` gives the best classification performance.\n"]},{"cell_type":"markdown","id":"8eb1eabf-3c7e-413e-9d1f-8c1e94ce16bd","metadata":{},"outputs":[],"source":["Here we could try K from 1 to 50, and store the aggregated `f1score` for each k into a list.\n"]},{"cell_type":"code","id":"3120f948-2918-4ace-b000-17175b613819","metadata":{},"outputs":[],"source":["# Try K from 1 to 50\nmax_k = 50\n# Create an empty list to store f1score for each k\nf1_scores = []"]},{"cell_type":"markdown","id":"f2250d9a-0394-427a-9c32-ba6983fdf196","metadata":{},"outputs":[],"source":["Then we will train 50 KNN classifiers with K ranged from 1 to 50.\n"]},{"cell_type":"code","id":"f70fe201-789d-4b96-86ff-1a2b6f0e6db4","metadata":{},"outputs":[],"source":["for k in range(1, max_k + 1):\n    # Create a KNN classifier\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Train the classifier\n    knn = knn.fit(X_train, y_train.values.ravel())\n    preds = knn.predict(X_test)\n    # Evaluate the classifier with f1score\n    f1 = f1_score(preds, y_test)\n    f1_scores.append((k, round(f1_score(y_test, preds), 4)))\n# Convert the f1score list to a dataframe\nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\nf1_results.set_index('K')"]},{"cell_type":"markdown","id":"77b2c71b-82e9-4d6f-a23b-8dfbcb292459","metadata":{},"outputs":[],"source":["This is a long list and different to analysis, so let's visualize the list using a linechart.\n"]},{"cell_type":"code","id":"e94db8a0-5e9f-4457-ba84-66a4208ccba2","metadata":{},"outputs":[],"source":["# Plot F1 results\nax = f1_results.plot(figsize=(12, 12))\nax.set(xlabel='Num of Neighbors', ylabel='F1 Score')\nax.set_xticks(range(1, max_k, 2));\nplt.ylim((0.85, 1))\nplt.title('KNN F1 Score')"]},{"cell_type":"markdown","id":"fa0bd30e-fffc-4780-acc7-8adfab2ec0c5","metadata":{},"outputs":[],"source":["As we can see from the F1 score linechart, the best `K` value is 5 with about `0.9691` f1score.\n"]},{"cell_type":"markdown","id":"7d520ea8-9fca-4752-a902-f0f29579c60b","metadata":{},"outputs":[],"source":["## Next steps\n"]},{"cell_type":"markdown","id":"43e977ae-3fbd-419e-9b05-0f106423dc6c","metadata":{},"outputs":[],"source":["Great! Now you have learned about and applied the KNN model to solve a real-world tumor type classification problem. You also tuned the KNN to find the best K value. Later, you will continue learning other popular classification models with different structures, assumptions, cost functions, and application scenarios.\n"]},{"cell_type":"markdown","id":"7b91b8b0-4ebc-4606-9200-ed5c8d587335","metadata":{},"outputs":[],"source":["## Authors\n"]},{"cell_type":"markdown","id":"30e07db8-f620-48dc-9b06-5117df72087f","metadata":{},"outputs":[],"source":["[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/)\n"]},{"cell_type":"markdown","id":"356d4fb4-708f-4365-9085-cc50c9468ac6","metadata":{},"outputs":[],"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"2a7a60f9-52e8-4516-aee2-c0f5453e40b1","metadata":{},"outputs":[],"source":["<!--## Change Log--!>\n"]},{"cell_type":"markdown","id":"5b9d38cc-c735-4af7-96ee-4ad721054bfa","metadata":{},"outputs":[],"source":["<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2021-11-9|1.0|Yan|Created the initial version|\n","|2022-3-29|1.1|Steve Hord|QA Pass|\n","--!>\n"]},{"cell_type":"markdown","id":"da2f6d19-2e22-4789-ab03-ef277a77b987","metadata":{},"outputs":[],"source":["Copyright Â© 2021 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.11.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"91c30c910c7945a998d3c1082d0ce375c10c5bb00d6c93723d696311d8d8b37b"},"nbformat":4,"nbformat_minor":4}