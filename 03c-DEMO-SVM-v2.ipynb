{"cells":[{"cell_type":"markdown","id":"6dee504d-5cb8-4121-a7c9-cb399ed6b070","metadata":{},"outputs":[],"source":["# Machine Learning Foundation\n","\n","## Course 3, Part c: Support Vector Machines DEMO\n"]},{"cell_type":"markdown","id":"25a8a56b-2698-468f-baea-636c9acf0622","metadata":{},"outputs":[],"source":["## Introduction\n","\n","We will be using the wine quality data set for these exercises. This data set contains various chemical properties of wine, such as acidity, sugar, pH, and alcohol. It also contains a quality metric (3-9, with highest being better) and a color (red or white). The name of the file is `Wine_Quality_Data.csv`.\n"]},{"cell_type":"code","id":"add50b2b-1184-455f-8dde-0a3a33bdad5f","metadata":{},"outputs":[],"source":["!pip install pandas\n!pip install numpy\n!pip install matplotlib\n!pip install seaborn\n!pip install scikit-learn"]},{"cell_type":"code","id":"4b0c9f32-88c5-4bf0-994b-5fcc1a241ed4","metadata":{},"outputs":[],"source":["def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns"]},{"cell_type":"markdown","id":"0aa200f4-b550-4e92-a413-71dfde36260a","metadata":{},"outputs":[],"source":["## Part 1: Setup\n","\n","* Import the data.\n","* Create the target variable `y` as a 1/0 column where 1 means red.\n","* Create a `pairplot` for the dataset.\n","* Create a bar plot showing the correlations between each column and `y`\n","* Pick the most 2 correlated fields (using the absolute value of correlations) and create `X`\n","* Use MinMaxScaler to scale `X`. Note that this will output a np.array. Make it a DataFrame again and rename the columns appropriately.\n"]},{"cell_type":"code","id":"9b8a3822-a3bd-44b9-b1ce-a0746baa48a8","metadata":{},"outputs":[],"source":["data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/Wine_Quality_Data.csv\", sep=',')\n"]},{"cell_type":"code","id":"bb49d4b1-cebb-4a71-832b-dcd919b2c044","metadata":{},"outputs":[],"source":["y = (data['color'] == 'red').astype(int)\nfields = list(data.columns[:-1])  # everything except \"color\"\ncorrelations = data[fields].corrwith(y)\ncorrelations.sort_values(inplace=True)\ncorrelations"]},{"cell_type":"code","id":"5568c100-2cc9-4635-ae78-668567b5fe01","metadata":{},"outputs":[],"source":["sns.set_context('talk')\n#sns.set_palette(palette)\nsns.set_style('white')"]},{"cell_type":"code","id":"f30ef4b9-dc97-4f19-8db4-ba03560f1a86","metadata":{},"outputs":[],"source":["sns.pairplot(data, hue='color')"]},{"cell_type":"code","id":"f70d0e8a-ca23-4ef4-8846-dcd87aa7def1","metadata":{},"outputs":[],"source":["ax = correlations.plot(kind='bar')\nax.set(ylim=[-1, 1], ylabel='pearson correlation');"]},{"cell_type":"code","id":"b3575bbc-edb9-46ed-82e3-e89e354b80c9","metadata":{},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n\nfields = correlations.map(abs).sort_values().iloc[-2:].index\nprint(fields)\nX = data[fields]\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X, columns=['%s_scaled' % fld for fld in fields])\nprint(X.columns)"]},{"cell_type":"markdown","id":"437e5164-e5de-46ce-be5a-eb1e61bc42e6","metadata":{},"outputs":[],"source":["## Part 2: Linear Decision Boundary\n","\n","Our goal is to look at the decision boundary of a LinearSVC classifier on this dataset. Check out [this example](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html#sphx-glr-auto-examples-svm-plot-iris-svc-py) in sklearn's documentation. \n","\n","* Fit a Linear Support Vector Machine Classifier to `X`, `y`.\n","* Pick 300 samples from `X`. Get the corresponding `y` value. Store them in variables `X_color` and `y_color`. This is because original dataset is too large and it produces a crowded plot.\n","* Modify `y_color` so that it has the value \"red\" instead of 1 and 'yellow' instead of 0.\n","* Scatter plot X_color's columns. Use the keyword argument \"color=y_color\" to color code samples.\n","* Use the code snippet below to plot the decision surface in a color coded way.\n","\n","```python\n","x_axis, y_axis = np.arange(0, 1, .005), np.arange(0, 1, .005)\n","xx, yy = np.meshgrid(x_axis, y_axis)\n","xx_ravel = xx.ravel()\n","yy_ravel = yy.ravel()\n","X_grid = pd.DataFrame([xx_ravel, yy_ravel]).T\n","y_grid_predictions = *[YOUR MODEL]*.predict(X_grid)\n","y_grid_predictions = y_grid_predictions.reshape(xx.shape)\n","ax.contourf(xx, yy, y_grid_predictions, cmap=colors, alpha=.3)\n","```\n","\n","With LinearSVC, it is easy to experiment with different parameter choices and see the decision boundary.\n"]},{"cell_type":"code","id":"d3f9ee50-715c-4d49-ba06-1324a5b968b2","metadata":{},"outputs":[],"source":["from sklearn.svm import LinearSVC\n\nLSVC = LinearSVC()\nLSVC.fit(X, y)\n\nX_color = X.sample(300, random_state=45)\ny_color = y.loc[X_color.index]\ny_color = y_color.map(lambda r: 'red' if r == 1 else 'yellow')\nax = plt.axes()\nax.scatter(\n    X_color.iloc[:, 0], X_color.iloc[:, 1],\n    color=y_color, alpha=1)\n# -----------\nx_axis, y_axis = np.arange(0, 1.005, .005), np.arange(0, 1.005, .005)\nxx, yy = np.meshgrid(x_axis, y_axis)\nxx_ravel = xx.ravel()\nyy_ravel = yy.ravel()\nX_grid = pd.DataFrame([xx_ravel, yy_ravel]).T\ny_grid_predictions = LSVC.predict(X_grid)\ny_grid_predictions = y_grid_predictions.reshape(xx.shape)\nax.contourf(xx, yy, y_grid_predictions, cmap=plt.cm.autumn_r, alpha=.3)\n# -----------\nax.set(\n    xlabel=fields[0],\n    ylabel=fields[1],\n    xlim=[0, 1],\n    ylim=[0, 1],\n    title='decision boundary for LinearSVC');"]},{"cell_type":"markdown","id":"fc8ce310-75e0-45f5-8af1-adac5a039f39","metadata":{},"outputs":[],"source":["## Part 3: Gaussian Kernel\n","\n","Let's now fit a Gaussian kernel SVC and see how the decision boundary changes.\n","\n","* Consolidate the code snippets in Question 2 into one function which takes in an estimator, `X` and `y`, and produces the final plot with decision boundary. The steps are:\n","    <ol>\n","     <li> Fit model\n","     <li> Get sample 300 records from X and the corresponding y's\n","     <li> Create grid, predict, plot using ax.contourf\n","     <li> Add on the scatter plot\n","    </ol>\n","* After copying and pasting code, the finished function uses the input `estimator` and not the LinearSVC model.\n","* For the following values of `gamma`, create a Gaussian Kernel SVC and plot the decision boundary.  \n","`gammas = [.5, 1, 2, 10]`\n","* Holding `gamma` constant, we plot the decision boundary for various values of `C`: \n","`[.1, 1, 10]`\n"]},{"cell_type":"code","id":"b9f3a4a5-73d5-4aef-8c22-945e29a09346","metadata":{},"outputs":[],"source":["def plot_decision_boundary(estimator, X, y):\n    estimator.fit(X, y)\n    X_color = X.sample(300)\n    y_color = y.loc[X_color.index]\n    y_color = y_color.map(lambda r: 'red' if r == 1 else 'yellow')\n    x_axis, y_axis = np.arange(0, 1, .005), np.arange(0, 1, .005)\n    xx, yy = np.meshgrid(x_axis, y_axis)\n    xx_ravel = xx.ravel()\n    yy_ravel = yy.ravel()\n    X_grid = pd.DataFrame([xx_ravel, yy_ravel]).T\n    y_grid_predictions = estimator.predict(X_grid)\n    y_grid_predictions = y_grid_predictions.reshape(xx.shape)\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.contourf(xx, yy, y_grid_predictions, cmap=plt.cm.autumn_r, alpha=.3)\n    ax.scatter(X_color.iloc[:, 0], X_color.iloc[:, 1], color=y_color, alpha=1)\n    ax.set(\n        xlabel=fields[0],\n        ylabel=fields[1],\n        title=str(estimator))"]},{"cell_type":"code","id":"ffb9ce55-e0f2-4dbe-bd60-f0bcf7681628","metadata":{},"outputs":[],"source":["from sklearn.svm import SVC\n\ngammas = [.5, 1, 2, 10]\nfor gamma in gammas:\n    SVC_Gaussian = SVC(kernel='rbf', gamma=gamma)\n    plot_decision_boundary(SVC_Gaussian, X, y)"]},{"cell_type":"code","id":"489085bb-2c03-42b4-b39e-f46a9bfd7c51","metadata":{},"outputs":[],"source":["Cs = [.1, 1, 10]\nfor C in Cs:\n    SVC_Gaussian = SVC(kernel='rbf', gamma=2, C=C)\n    plot_decision_boundary(SVC_Gaussian, X, y)"]},{"cell_type":"markdown","id":"4b7bc265-bdca-495e-aaa9-f91b6e9fecfa","metadata":{},"outputs":[],"source":["## Part 4: Comparing Kernel Execution Times\n","\n","In this exercise, we will compare the fitting times between SVC vs Nystroem with rbf kernel.  \n","<br><br>\n","Jupyter Notebooks provide a useful magic function **`%timeit`** which executes a line and prints out the time it took to fit. If we type **`%%timeit`** in the beginning of the cell, it will output the execution time.\n","\n","We proceed with the following steps:\n","* Create `y` from data.color, and `X` from the rest of the columns.\n","* Use `%%timeit` to get the time for fitting an SVC with rbf kernel.\n","* Use `%%timeit` to get the time for the following: fit_transform the data with Nystroem and then fit a SGDClassifier.\n","\n","Nystroem+SGD will take much less to fit. This difference will be more pronounced if the dataset was bigger.\n","\n","* Make 5 copies of X and concatenate them\n","* Make 5 copies of y and concatenate them\n","* Compare the time it takes to fit the both methods above\n"]},{"cell_type":"code","id":"e013999b-c015-4453-a4d0-eaecac87d06c","metadata":{},"outputs":[],"source":["from sklearn.kernel_approximation import Nystroem\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\n\ny = data.color == 'red'\nX = data[data.columns[:-1]]\n\nkwargs = {'kernel': 'rbf'}\nsvc = SVC(**kwargs)\nnystroem = Nystroem(**kwargs)\nsgd = SGDClassifier()"]},{"cell_type":"code","id":"f694dc70-7992-4005-94a3-f2de04581224","metadata":{},"outputs":[],"source":["%%timeit\nsvc.fit(X, y)"]},{"cell_type":"code","id":"4d0cca24-1032-4b14-9927-d99c98aa56bc","metadata":{},"outputs":[],"source":["%%timeit\nX_transformed = nystroem.fit_transform(X)\nsgd.fit(X_transformed, y)"]},{"cell_type":"code","id":"c88ffcd1-15d5-4760-b0da-34b42b4798ec","metadata":{},"outputs":[],"source":["X2 = pd.concat([X]*5)\ny2 = pd.concat([y]*5)\n\nprint(X2.shape)\nprint(y2.shape)"]},{"cell_type":"code","id":"be8d3369-b8ad-476d-a719-b4fdf8de0d2f","metadata":{},"outputs":[],"source":["%timeit svc.fit(X2, y2)"]},{"cell_type":"code","id":"f5ae9f10-4307-49c8-9ac8-1781d5ed00df","metadata":{},"outputs":[],"source":["%%timeit\nX2_transformed = nystroem.fit_transform(X2)\nsgd.fit(X2_transformed, y2)"]},{"cell_type":"markdown","id":"51a2261e-b6bc-443f-851f-d566e9153e50","metadata":{},"outputs":[],"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.11.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"2c34a867a9d81be4ba7480d52c4d46ec0b01393f93e5e43f00088065df668e95"},"nbformat":4,"nbformat_minor":4}